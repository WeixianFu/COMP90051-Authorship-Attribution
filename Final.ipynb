{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11312da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC, LinearSVC, SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from BaseSVDD import BaseSVDD\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import sparse\n",
    "from sklearn.multioutput import ClassifierChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0836f4d",
   "metadata": {},
   "source": [
    "# Data R/W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a152538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Openinng the JSON file\n",
    "openAllData = open('data/train.json');\n",
    "openTestData = open('data/test.json');\n",
    "\n",
    "# returns JSON object as a dictionary\n",
    "allData = json.load(openAllData);\n",
    "testData = json.load(openTestData);\n",
    "\n",
    "openAllData.close();\n",
    "openTestData.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415213bb",
   "metadata": {},
   "source": [
    "Treat year and venue also as keywords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b15eccb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "allData_str_list = []\n",
    "for k1, v1 in allData.items():\n",
    "    templist = []\n",
    "    templist.append(\"year_\" + str(v1[\"year\"]))\n",
    "    templist.append(\"venue_\" + str(v1[\"venue\"]))\n",
    "    templist.extend([\"keywords_\" + str(keyword) for keyword in v1[\"keywords\"]])\n",
    "#     templist.extend([\"author_\" + str(author) for author in v1[\"author\"]])\n",
    "#     templist.extend(v1[\"author\"])\n",
    "    tempstr = ','.join(templist)\n",
    "    allData_str_list.append([tempstr, v1[\"author\"]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d564b8",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c02b69b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_allData_str = [row[0] for row in allData_str_list]\n",
    "y_allData_str = [row[1] for row in allData_str_list]\n",
    "\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(x_allData_str, y_allData_str, \n",
    "                                                    test_size=2000, random_state=90051, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80c854e",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a926f555",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3763eb72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ThomasZhao\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(tokenizer=<function string_comma_split at 0x00000253874BA8B0>,\n",
       "                vocabulary={'keywords_0': 490, 'keywords_1': 491,\n",
       "                            'keywords_10': 500, 'keywords_100': 590,\n",
       "                            'keywords_101': 591, 'keywords_102': 592,\n",
       "                            'keywords_103': 593, 'keywords_104': 594,\n",
       "                            'keywords_105': 595, 'keywords_106': 596,\n",
       "                            'keywords_107': 597, 'keywords_108': 598,\n",
       "                            'keywords_109': 599, 'keywords_11': 501,\n",
       "                            'keywords_110': 600, 'keywords_111': 601,\n",
       "                            'keywords_112': 602, 'keywords_113': 603,\n",
       "                            'keywords_114': 604, 'keywords_115': 605,\n",
       "                            'keywords_116': 606, 'keywords_117': 607,\n",
       "                            'keywords_118': 608, 'keywords_119': 609,\n",
       "                            'keywords_12': 502, 'keywords_120': 610,\n",
       "                            'keywords_121': 611, 'keywords_122': 612,\n",
       "                            'keywords_123': 613, 'keywords_124': 614, ...})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_YEAR = 20\n",
    "START_YEAR = 2000\n",
    "NUM_VENUE = 470\n",
    "NUM_KEYWORDS = 500\n",
    "NUM_AUTHOR = 2302\n",
    "vocabulary = {}\n",
    "vocab_index = 0\n",
    "for i in range(NUM_YEAR):\n",
    "    vocabulary[\"year_\"+str(START_YEAR+i)] = vocab_index\n",
    "    vocab_index+=1\n",
    "for i in range(NUM_VENUE):\n",
    "    vocabulary[\"venue_\"+str(i)] = vocab_index\n",
    "    vocab_index+=1\n",
    "for i in range(NUM_KEYWORDS):\n",
    "    vocabulary[\"keywords_\"+str(i)] = vocab_index\n",
    "    vocab_index+=1\n",
    "    \n",
    "def string_comma_split(string):\n",
    "    return string.split(\",\")\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=string_comma_split, vocabulary=vocabulary)\n",
    "\n",
    "tfidf_vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ba7f489",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = tfidf_vectorizer.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2e06471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_transform(y_list):\n",
    "    y_transform = np.zeros((len(y_list), 2302))\n",
    "    for i in range(len(y_list)):\n",
    "        for j in y_list[i]:\n",
    "            y_transform[i][j] = 1\n",
    "    y_transform = sparse.csr_matrix(y_transform)\n",
    "    return y_transform\n",
    "\n",
    "y_train_transform = y_transform(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1647f1",
   "metadata": {},
   "source": [
    "### Author PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05eaf13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_occurance(author_list):\n",
    "    tempdict = defaultdict(int)\n",
    "    \n",
    "    for authors in author_list:\n",
    "        for index1 in range(len(authors)):\n",
    "            for index2 in range(index1, len(authors)):\n",
    "                key = tuple(sorted([authors[index1], authors[index2]]))\n",
    "                tempdict[key] += 1\n",
    "    \n",
    "    vocab = list(range(2302))\n",
    "    df = pd.DataFrame(data=np.zeros((2302, 2302), dtype=np.int16),index=vocab,columns=vocab)\n",
    "    for key, value in tempdict.items():\n",
    "        df.at[key[0], key[1]] = value\n",
    "        df.at[key[1], key[0]] = value\n",
    "    return df\n",
    "\n",
    "def pmi(df, positive=True):\n",
    "    col_totals = df.sum(axis=0)\n",
    "#     total = col_totals.sum()\n",
    "    row_totals = df.sum(axis=1)\n",
    "    total = np.zeros((len(row_totals), len(col_totals)))\n",
    "    \n",
    "    for i in range(len(row_totals)):\n",
    "        for j in range(len(col_totals)):\n",
    "            total[i][j] = row_totals[i] + col_totals[j] + 0.1\n",
    "            \n",
    "    for i in range(len(total)):\n",
    "        for j in range(len(total[i])):\n",
    "            if total[i][j] == 0:\n",
    "                total[i][j] = math.inf\n",
    "    expected = np.outer(row_totals, col_totals) / total\n",
    "    \n",
    "    for i in range(len(expected)):\n",
    "        for j in range(len(expected[i])):\n",
    "            if expected[i][j] == 0:\n",
    "                expected[i][j] = math.inf\n",
    "    df = df / expected\n",
    "    # Silence distracting warnings about log(0):\n",
    "    with np.errstate(divide='ignore'):\n",
    "        df = np.log(df)\n",
    "    df[np.isinf(df)] = -1  # log(0) = 0\n",
    "    if positive:\n",
    "        df[df < 0] = 0.0\n",
    "#     print(total)\n",
    "    return df\n",
    "\n",
    "def freq_proba(df):\n",
    "    author_totals = df.sum(axis=1)\n",
    "    freq_df = copy.deepcopy(df)\n",
    "    freq_df.to_numpy()\n",
    "    for row in range(len(freq_df)):\n",
    "        for col in range(len(freq_df[row])):\n",
    "            freq_df[row][col] == freq_df[row][col]/author_totals[col]\n",
    "    return freq_df\n",
    "\n",
    "def scale_to_05_1(num):\n",
    "    return (num**2 + 1)/2\n",
    "\n",
    "def sigmoid_my(num):\n",
    "    return 1/(1 + np.exp(-num))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81bfc312",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = co_occurance(y_train)\n",
    "ppmi = pmi(df, positive=True)\n",
    "ppmi_sigmoid = sigmoid_my(ppmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc747a7a",
   "metadata": {},
   "source": [
    "### Author post proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69a6ea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = co_occurance(y_allData_str)\n",
    "freq_df = freq_proba(df)\n",
    "freq_df_scale = scale_to_05_1(freq_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb773c72",
   "metadata": {},
   "source": [
    "# Traininig stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4df75e",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea8eaae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(), n_jobs=-1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_LR = OneVsRestClassifier(LogisticRegression(), n_jobs=-1)\n",
    "clf_LR.fit(X_train_tfidf, y_train_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e860b9ec",
   "metadata": {},
   "source": [
    "### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ce7c56f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LinearSVC(), n_jobs=-1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_LSVC = OneVsRestClassifier(LinearSVC(), n_jobs=-1)\n",
    "clf_LSVC.fit(X_train_tfidf, y_train_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dd5ae1",
   "metadata": {},
   "source": [
    "# Evaluation stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccecae9b",
   "metadata": {},
   "source": [
    "### Preparing dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55272e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev_tfidf = tfidf_vectorizer.transform(X_dev)\n",
    "\n",
    "# LR\n",
    "y_dev_decision_function_LR = clf_LR.decision_function(X_dev_tfidf)\n",
    "y_dev_proba_raw_LR = sigmoid_my(y_dev_decision_function_LR)\n",
    "\n",
    "# LSVC\n",
    "y_dev_decision_function_LSVC = clf_LSVC.decision_function(X_dev_tfidf)\n",
    "y_dev_proba_raw_LSVC = sigmoid_my(y_dev_decision_function_LSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e30f79f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev_get_author_target(y_dev):\n",
    "    target_list = []\n",
    "    coauthor = []\n",
    "    for author_list in y_dev:\n",
    "        templist = copy.deepcopy(author_list)\n",
    "        [target] = random.sample(author_list, 1)\n",
    "        target_list.append(target)\n",
    "        templist.remove(target)\n",
    "        coauthor.append(templist)\n",
    "    with_false = copy.deepcopy(target_list)\n",
    "    for i in range(0, 2000, 2):\n",
    "        all = set(list(range(2032)))\n",
    "        temp = set(y_dev[i])\n",
    "        [with_false[i]] = random.sample(list(all - temp), 1)\n",
    "    return target_list, coauthor, with_false\n",
    "\n",
    "true_target_list, coauthor_list, target_list = dev_get_author_target(y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f66be69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_freq(freq_df_scale, coauthor_list, target):\n",
    "    max_freq = 0.5\n",
    "    for coauthor in coauthor_list:\n",
    "        max_freq = max(max_freq, freq_df_scale[target][coauthor])\n",
    "#     if max_ppmi>0.5:\n",
    "#         print(max_ppmi)\n",
    "    return max_freq\n",
    "\n",
    "def max_ppmi(data_ppmi_sigmoid, coauthor_list, target):\n",
    "    max_ppmi = 0.5\n",
    "    for coauthor in coauthor_list:\n",
    "        max_ppmi = max(max_ppmi, data_ppmi_sigmoid[target][coauthor])\n",
    "#     if max_ppmi>0.5:\n",
    "#         print(max_ppmi)\n",
    "    return max_ppmi\n",
    "    \n",
    "def raw_to_final(target_list, coauthor_list, data_proba_row, data_ppmi_sigmoid):\n",
    "    results = []\n",
    "    for i in range(len(target_list)):\n",
    "        target = target_list[i]\n",
    "        results.append(data_proba_row[i][target] * max_freq(freq_df_scale, coauthor_list[i], target)) \n",
    "        # if u want to use pmi to adjust the proba, use func max_ppmi instead of max_freq\n",
    "    return results     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1630c67",
   "metadata": {},
   "source": [
    "### Compute AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445668fb",
   "metadata": {},
   "source": [
    "### LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0939f043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9140430000000002"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_LR = []\n",
    "y_result_LR = []\n",
    "for i in range(len(target_list)):\n",
    "    results_LR.append(y_dev_proba_raw_LR[i][target_list[i]])\n",
    "    if target_list[i] == true_target_list[i]:\n",
    "        y_result_LR.append(1)\n",
    "    else:\n",
    "        y_result_LR.append(0)\n",
    "\n",
    "results_LR = raw_to_final(target_list, coauthor_list, y_dev_proba_raw_LR, freq_df_scale)\n",
    "        \n",
    "roc_auc_score(y_result_LR, results_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aec536",
   "metadata": {},
   "source": [
    "### LSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0a62c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9388099999999999"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_LSVC = []\n",
    "y_result_LSVC = []\n",
    "for i in range(len(target_list)):\n",
    "    results_LSVC.append(y_dev_proba_raw_LSVC[i][target_list[i]])\n",
    "    if target_list[i] == true_target_list[i]:\n",
    "        y_result_LSVC.append(1)\n",
    "    else:\n",
    "        y_result_LSVC.append(0)\n",
    "\n",
    "results_LSVC = raw_to_final(target_list, coauthor_list, y_dev_proba_raw_LSVC, freq_df_scale)    \n",
    "        \n",
    "roc_auc_score(y_result_LSVC, results_LSVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b7ab0e",
   "metadata": {},
   "source": [
    "The LinearSVC model performs better than the LogisticRegression model. Therefore, we select LSVC as the base model for further optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c71904",
   "metadata": {},
   "source": [
    "# Parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c859c92",
   "metadata": {},
   "source": [
    "TBA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbead459",
   "metadata": {},
   "source": [
    "# Experiment - Classifier chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df3ffe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "chains = [ClassifierChain(LinearSVC(), order=\"random\", random_state=i) for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "158cacdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chaining order 1: AUC = 0.9350169999999999\n",
      "Chaining order 2: AUC = 0.9357959999999999\n",
      "Chaining order 3: AUC = 0.936123\n",
      "Chaining order 4: AUC = 0.9379749999999999\n",
      "Chaining order 5: AUC = 0.935178\n"
     ]
    }
   ],
   "source": [
    "chain_order = 1 \n",
    "for chain in chains:\n",
    "    chain.fit(X_train_tfidf, y_train_transform.toarray())\n",
    "    y_dev_decision_function = chain.decision_function(X_dev_tfidf)\n",
    "    y_dev_proba_raw = sigmoid_my(y_dev_decision_function)\n",
    "    \n",
    "    results = []\n",
    "    y_result = []\n",
    "    for i in range(len(target_list)):\n",
    "        results.append(y_dev_proba_raw[i][target_list[i]])\n",
    "        if target_list[i] == true_target_list[i]:\n",
    "            y_result.append(1)\n",
    "        else:\n",
    "            y_result.append(0)\n",
    "\n",
    "    results = raw_to_final(target_list, coauthor_list, y_dev_proba_raw, freq_df_scale)    \n",
    "\n",
    "    auc = roc_auc_score(y_result, results)\n",
    "    \n",
    "    print(\"Chaining order \" + str(chain_order) + \": AUC = \" + str(auc))\n",
    "    chain_order += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca018a3",
   "metadata": {},
   "source": [
    "Classifier chaining approach performs no better than the much easier and much faster binary relevance one vs rest approach. In addition, there is also very small variations regardless of different orders of chaining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f230a8",
   "metadata": {},
   "source": [
    "# Test set prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1f4a416e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf_LR # best classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b17c6701",
   "metadata": {},
   "outputs": [],
   "source": [
    "testData_str_list = []\n",
    "for k1, v1 in testData.items():\n",
    "    templist = []\n",
    "    templist.append(\"year_\" + str(v1[\"year\"]))\n",
    "    templist.append(\"venue_\" + str(v1[\"venue\"]))\n",
    "    templist.extend([\"keywords_\" + str(keyword) for keyword in v1[\"keywords\"]])\n",
    "    tempstr = ','.join(templist)\n",
    "    testData_str_list.append([tempstr,v1[\"target\"], v1[\"coauthor\"]])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58522ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_testData_str = [row[0] for row in testData_str_list]\n",
    "y_testData = [row[1] for row in testData_str_list]\n",
    "coauthor_testData = [row[2] for row in testData_str_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "01ed7c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf = tfidf_vectorizer.transform(x_testData_str)\n",
    "y_test_decision_function = clf.decision_function(X_test_tfidf)\n",
    "y_test_proba_raw = sigmoid_my(y_test_decision_function)\n",
    "results = raw_to_final(y_testData, coauthor_testData, y_test_proba_raw, freq_df_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a8ab4b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "header = ['Id','Predicted']\n",
    "data = []\n",
    "idNumber = 0\n",
    "for i in results:\n",
    "    data.append([idNumber,i])\n",
    "    idNumber = idNumber+1\n",
    "\n",
    "filename = 'multi_label_LinearSVM_FREQ.csv'\n",
    "with open(filename, 'w', newline=\"\") as file:\n",
    "    csvwriter = csv.writer(file)\n",
    "    csvwriter.writerow(header)\n",
    "    csvwriter.writerows(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
